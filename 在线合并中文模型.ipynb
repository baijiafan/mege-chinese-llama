{"cells":[{"cell_type":"markdown","metadata":{"id":"B1c96_k3MahN"},"source":["# è½¬æ¢å¹¶é‡åŒ–ä¸­æ–‡Alpaca Plusæ¨¡å‹\n","\n","è¿™æ˜¯å’ŒçŸ¥ä¹ç³»åˆ—ç²¾è°ƒæ–‡ç« \n","\n","* [å°è¯•å¯¹Chinese-LLaMA-Alpacaè¿›è¡Œå¾®è°ƒ-å‡†å¤‡ç¯‡](https://zhuanlan.zhihu.com/p/630522733)\n","* [å°è¯•å¯¹Chinese-LLaMA-Alpacaè¿›è¡Œå¾®è°ƒ-æ•°æ®å‡†å¤‡ç¯‡](https://zhuanlan.zhihu.com/p/630544641)\n","* [å°è¯•å¯¹Chinese-LLaMA-Alpacaè¿›è¡Œå¾®è°ƒ-å®è·µç¯‡](https://zhuanlan.zhihu.com/p/632291297)\n","\n","\n","å¯¹åº”çš„åœ¨çº¿è„šæœ¬ã€‚\n","\n","å’Œå®˜æ–¹è„šæœ¬ç›¸æ¯”å¢åŠ äº†å¯¹äºæ¨¡å‹çš„å­˜å‚¨è¿‡ç¨‹ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨åç»­çš„æ­¥éª¤ä¸­è¿›è¡Œè¿›ä¸€æ­¥çš„ç²¾è°ƒã€‚\n","\n","å®˜æ–¹è„šæœ¬ä¸­æœ‰å¾ˆå¤šå…³äºGoogle ColabèŠ±è´¹ï¼Œç»éªŒç­‰çš„ä»‹ç»ï¼Œå¤§å®¶å¯ä»¥å¤šå¤šæŸ¥é˜…\n","\n","[å®˜æ–¹è„šæœ¬](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/notebooks/convert_and_quantize_chinese_alpaca_plus.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"vScqHD_jMFOV"},"source":["## å®‰è£…ç›¸å…³ä¾èµ–\n","\n","å’Œå®˜æ–¹ä¸åŒçš„æ˜¯è¿™ä¸ªåœ°æ–¹ä¼šå¤šå®‰è£…ä¸€ä¸ªmpi4pyï¼Œç”¨äºç²¾è°ƒé˜¶æ®µ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5WKFJXIL6ZU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685505190896,"user_tz":-480,"elapsed":99043,"user":{"displayName":"åˆ˜é“­","userId":"16932740466479782785"}},"outputId":"9dbe5bfb-9c6d-42ed-f20a-aa029c647cac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/peft\n","  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-ad6af76y\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-ad6af76y\n","  Resolved https://github.com/huggingface/peft to commit 3714aa2fff158fdfa637b2b65952580801d890b2\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (6.0)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (2.0.1+cu118)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (4.29.2)\n","Collecting accelerate (from peft==0.4.0.dev0)\n","  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (16.0.5)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (0.14.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0.dev0) (2023.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.4.0.dev0) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.4.0.dev0) (1.3.0)\n","Building wheels for collected packages: peft\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=56971 sha256=e7bc42160d8d1a0036d4c770facb8473b0cf65b56faf247f4df94c3bda6419a3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-hly4p0cv/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\n","Successfully built peft\n","Installing collected packages: accelerate, peft\n","Successfully installed accelerate-0.19.0 peft-0.4.0.dev0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mpi4py\n","  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: mpi4py\n","  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp310-cp310-linux_x86_64.whl size=3365632 sha256=aa88fcf7382ac28fc7dc73dca8f4c0ac35a405cebd55f128847b47b66d844eb1\n","  Stored in directory: /root/.cache/pip/wheels/e8/1b/b5/97ec4cfccdde26e0f3590ad6e09a5242d508dff09704ef86c1\n","Successfully built mpi4py\n","Installing collected packages: mpi4py\n","Successfully installed mpi4py-3.1.4\n"]}],"source":["!pip install torch\n","!pip install transformers\n","!pip install git+https://github.com/huggingface/peft\n","!pip install sentencepiece\n","!pip install mpi4py"]},{"cell_type":"markdown","metadata":{"id":"ygb1xFIMNQKw"},"source":["## å…‹éš†ç›®å½•å’Œä»£ç "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCEJh7NJNXz9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685505223718,"user_tz":-480,"elapsed":1926,"user":{"displayName":"åˆ˜é“­","userId":"16932740466479782785"}},"outputId":"239e9129-e6b5-438f-fabd-1b6def31da4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Chinese-LLaMA-Alpaca'...\n","remote: Enumerating objects: 1153, done.\u001b[K\n","remote: Counting objects: 100% (345/345), done.\u001b[K\n","remote: Compressing objects: 100% (149/149), done.\u001b[K\n","remote: Total 1153 (delta 216), reused 266 (delta 193), pack-reused 808\u001b[K\n","Receiving objects: 100% (1153/1153), 18.51 MiB | 13.65 MiB/s, done.\n","Resolving deltas: 100% (678/678), done.\n"]}],"source":["!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca"]},{"cell_type":"markdown","source":["# å®ŒæˆGoogle Driveçš„åŠ è½½"],"metadata":{"id":"2QFYReeYgq2D"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUbLnPr3b3A-","executionInfo":{"status":"ok","timestamp":1685505221796,"user_tz":-480,"elapsed":30908,"user":{"displayName":"åˆ˜é“­","userId":"16932740466479782785"}},"outputId":"d87a29d1-c6d8-4c91-f04e-f37b3bb708de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"nIyxX0DSNsgQ"},"source":["# åˆå¹¶æ¨¡å‹ï¼ˆAlpaca-Plus-7Bï¼‰\n","\n","## æ³¨æ„ï¼šæœ¬æ­¥éª¤åªæ‰§è¡Œä¸€æ¬¡\n","\n","æœ¬æ­¥éª¤æ˜¯å¯¹æ¨¡å‹è¿›è¡Œåˆå¹¶ï¼Œè¯·æ³¨æ„ï¼Œè¿™ä¸ªæ­¥éª¤åœ¨æˆ‘ä»¬æ•´ä½“ç²¾è°ƒæ­¥éª¤ä¸­åªä¼šæ‰§è¡Œä¸€æ¬¡ï¼Œä»ç¬¬äºŒæ¬¡å¼€å§‹ï¼Œéƒ½ç›´æ¥è·³è¿‡è¿™ä¸ªæ­¥éª¤ï¼Œç›´æ¥ä»Google Driveä¸­åŠ è½½æ¨¡å‹\n","\n","## æé†’ï¼šé™¤ç²¾è°ƒé˜¶æ®µå¤–ï¼Œå…¶ä»–æ­¥éª¤å¯é€‰ç”¨èŠ±è´¹è¾ƒå°‘çš„æœºå‹\n","\n","å¦‚æœä½ å·²ç»äº†è§£åˆ°æ•´ä¸ªæµç¨‹å…¶å®åˆ†æˆäº†ç¬¬ä¸€æ¬¡åˆå¹¶ï¼Œè·å¾—ä¸­æ–‡åˆå¹¶æ¨¡å‹ï¼Œå’Œä¹‹åçš„ç²¾è°ƒé˜¶æ®µï¼Œé‚£ä¹ˆä½ ç¬¬ä¸€æ¬¡æ‰§è¡Œçš„æ—¶å€™å°±å¯ä»¥ä¸ç”¨é€‰æ‹©å¤ªè´µçš„ç¡¬ä»¶ï¼Œåˆå¹¶ä¸éœ€è¦å¤ªå¤šGPUèµ„æº\n","\n","## å®˜æ–¹æç¤º\n","\n","ğŸ’¡ è½¬æ¢13Bæ¨¡å‹æç¤ºï¼š\n","- è¯·å°†å‚æ•°`--base_model`å’Œ`--lora_model`ä¸­çš„çš„`7b`æ”¹ä¸º`13b`å³å¯\n","- **å…è´¹ç”¨æˆ·å¿…é¡»å¢åŠ ä¸€ä¸ªå‚æ•°`--offload_dir`ä»¥ç¼“è§£å†…å­˜å‹åŠ›**ï¼Œä¾‹å¦‚`--offload_dir ./offload_temp`\n","\n","è¯¥è¿‡ç¨‹æ¯”è¾ƒè€—æ—¶ï¼ˆä¸‹è½½+è½¬æ¢ï¼‰ï¼Œéœ€è¦å‡ åˆ†é’Ÿåˆ°åå‡ åˆ†é’Ÿä¸ç­‰ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚\n","è½¬æ¢å¥½çš„æ¨¡å‹å­˜æ”¾åœ¨`alpaca-combined`ç›®å½•ã€‚\n","å¦‚æœä½ ä¸éœ€è¦é‡åŒ–æ¨¡å‹ï¼Œé‚£ä¹ˆåˆ°è¿™ä¸€æ­¥å°±ç»“æŸäº†ã€‚"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AV4EW5hNhVV"},"outputs":[],"source":["!python ./Chinese-LLaMA-Alpaca/scripts/merge_llama_with_chinese_lora.py \\\n","    --base_model decapoda-research/llama-7b-hf \\\n","    --lora_model ziqingyang/chinese-llama-plus-lora-7b,ziqingyang/chinese-alpaca-plus-lora-7b \\\n","    --output_type huggingface \\\n","    --output_dir alpaca-combined"]},{"cell_type":"markdown","source":["## ä¿å­˜åˆå¹¶å¥½çš„æ¨¡å‹\n","\n","è¿™ä¸ªæ­¥éª¤å°†åˆå¹¶å¥½çš„æ¨¡å‹æ‹·è´ä¿å­˜åˆ°Google Driveä¸­ï¼Œä»¥ä¾¿åœ¨ä»¥åçš„ç²¾è°ƒå·¥ä½œä¸­ä½¿ç”¨ï¼Œè¯·æ³¨æ„ï¼Œæ¨¡å‹æ¯”è¾ƒå¤§ï¼Œéœ€è¦æ”¶è´¹ç‰ˆæœ¬æ‰èƒ½å­˜ä¸‹ã€‚\n","\n","ä¸ºäº†ä»¥åå’Œç²¾è°ƒæƒé‡å†æ¬¡åˆå¹¶ï¼Œè¿™ä¸ªåˆå¹¶æ¨¡å‹è¯·ä»Google Driveä¸­ä¸‹è½½ä¸€ä»½åˆ°æœ¬åœ°ï¼Œä»¥åå°±å¯ä»¥åœ¨æœ¬åœ°åˆå¹¶æœ€ç»ˆæ¨¡å‹äº†ï¼Œå…·ä½“åŸå› åœ¨æ–‡æœ«ã€‚"],"metadata":{"id":"r814pb-6g0u9"}},{"cell_type":"code","source":["!cp -fr /content/alpaca-combined /content/drive/MyDrive/"],"metadata":{"id":"BkKXEDldhIUL","executionInfo":{"status":"ok","timestamp":1685494405725,"user_tz":-480,"elapsed":6,"user":{"displayName":"åˆ˜é“­","userId":"16932740466479782785"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d3b7d391-0cf9-4ff5-f7ac-0c7eccf73249"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/alpaca-combined': No such file or directory\n"]}]},{"cell_type":"markdown","source":["# ç›´æ¥ä»driver load\n","\n","## æ³¨æ„ï¼Œé™¤ç¬¬ä¸€æ¬¡å¤–éƒ½æ‰§è¡Œä»¥ä¸‹æ­¥éª¤"],"metadata":{"id":"fn7f1x0Xbun7"}},{"cell_type":"markdown","source":["æ‹·è´æ¨¡å‹æ–‡ä»¶"],"metadata":{"id":"JDFgVLV6b5Gv"}},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/alpaca-combined /content/"],"metadata":{"id":"T9oVB8l_b9EM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["æ‹·è´ç²¾è°ƒæ–‡ä»¶\n","\n","æ³¨æ„ï¼šç²¾è°ƒæ–‡ä»¶è¿™é‡Œå‡è®¾æ˜¯ft.jsonï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„æ–‡ä»¶åï¼Œè°ƒæ•´ç¬¬ä¸€ä¸ªft.jsonï¼Œç›®æ ‡åç§°è¯·ä¸è¦æ”¹åŠ¨ï¼Œä»¥ä¾¿åç»­æ­¥éª¤ä¸­å¯¹é½"],"metadata":{"id":"rOMweaUjhT0G"}},{"cell_type":"code","source":["!mkdir -p datas\n","!cp /content/drive/MyDrive/ft.json /content/datas/ft.json"],"metadata":{"id":"jJFAYFNLcneA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLkuRAo9Vkb1"},"source":["## è¿›è¡Œè¿›ä¸€æ­¥ç²¾è°ƒ\n"]},{"cell_type":"code","source":["!pip install datasets\n","!pip install deepspeed"],"metadata":{"id":"_WmxcC255Rtv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685505388835,"user_tz":-480,"elapsed":30711,"user":{"displayName":"åˆ˜é“­","userId":"16932740466479782785"}},"outputId":"3201f87e-5a1b-465c-84a1-ef09c4886753"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Collecting responses<0.19 (from datasets)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting deepspeed\n","  Downloading deepspeed-0.9.2.tar.gz (779 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m779.3/779.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting hjson (from deepspeed)\n","  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja (from deepspeed)\n","  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n","Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.10.7)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.0.1+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.65.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0->deepspeed) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.12.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->deepspeed) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->deepspeed) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n","Building wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.9.2-py3-none-any.whl size=811220 sha256=aec7458a622095bd2c8161947d3dd3042154df0ec8af24564a7c7138e3eb804e\n","  Stored in directory: /root/.cache/pip/wheels/a6/d2/b1/b15210b5dc024bab4eccbac2148db29959fe01fe6042557d07\n","Successfully built deepspeed\n","Installing collected packages: ninja, hjson, deepspeed\n","Successfully installed deepspeed-0.9.2 hjson-3.1.0 ninja-1.11.1\n"]}]},{"cell_type":"markdown","source":["æ‰§è¡Œå‰ï¼Œè¯·å®Œæˆä»¥ä¸‹è®¾ç½®ï¼š\n","\n","* åœ¨èœå•ï¼šä»£ç æ‰§è¡Œç¨‹åºä¸­ç‚¹å‡»æ›´æ”¹è¿è¡Œæ—¶ç±»å‹\n","* åœ¨ç¡¬ä»¶åŠ é€Ÿå™¨ä¸­é€‰æ‹©GPU\n","* åœ¨GPUç±»å‹ä¸­é€‰æ‹©åˆé€‚çš„ç±»å‹ï¼Œæˆ‘ä¸€èˆ¬ä½¿ç”¨A100ï¼Œå…¶ä»–å‹å·ä¼¼ä¹å†…å­˜ä¸è¶³ã€‚\n","\n","åœ¨æˆ‘çš„ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œä¸€æ¬¡ç²¾è°ƒå¤§æ¦‚20åˆ†é’Ÿå·¦å³ï¼ŒèŠ±è´¹ç‚¹æ•°5-6ä¸ªç‚¹ï¼Œ100ä¸ªç‚¹9.99ï¼Œä¸€æ¬¡å¤§æ¦‚åˆäººæ°‘å¸3ï¼Œ4å—é’±å§ï¼Œå½“ç„¶è¿™ä¸ªå–å†³äºä½ çš„æ•°æ®é‡å¤§å°"],"metadata":{"id":"vTYm60AcfQaT"}},{"cell_type":"code","source":["!cd ./Chinese-LLaMA-Alpaca/scripts && python run_clm_sft_with_peft.py --deepspeed ds_zero2_no_offload.json --model_name_or_path /content/alpaca-combined --tokenizer_name_or_path /content/alpaca-combined --dataset_dir /content/datas --validation_split_percentage 0.001 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --do_train --do_eval --seed $RANDOM --max_steps 100 --lr_scheduler_type cosine --learning_rate 1e-4 --warmup_ratio 0.03 --weight_decay 0 --logging_strategy steps --logging_steps 10 --save_strategy steps --save_total_limit 3 --evaluation_strategy steps --eval_steps 250 --save_steps 500 --gradient_accumulation_steps 1 --preprocessing_num_workers 8 --max_seq_length 512 --output_dir /content/chinese_sfted --overwrite_output_dir --ddp_timeout 30000 --logging_first_step True --lora_rank 8 --lora_alpha 32 --trainable \"q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj\" --modules_to_save \"embed_tokens,lm_head\" --lora_dropout 0.05 --fp16 --torch_dtype float16 --validation_file /content/datas/ft.json --force_resize_embeddings False --gradient_checkpointing --ddp_find_unused_parameters False"],"metadata":{"id":"ZX3vAnJP7cf7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685505597022,"user_tz":-480,"elapsed":208196,"user":{"displayName":"åˆ˜é“­","userId":"16932740466479782785"}},"outputId":"f091b16c-3626-4c47-c4f2-63f9e3f2ee74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-31 03:56:34.996177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","05/31/2023 03:56:43 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n","[INFO|configuration_utils.py:667] 2023-05-31 03:56:43,004 >> loading configuration file /content/alpaca-combined/config.json\n","[INFO|configuration_utils.py:725] 2023-05-31 03:56:43,004 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"/content/alpaca-combined\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 2048,\n","  \"max_sequence_length\": 2048,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"pad_token_id\": -1,\n","  \"rms_norm_eps\": 1e-06,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.29.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 49954\n","}\n","\n","[INFO|tokenization_utils_base.py:1808] 2023-05-31 03:56:43,005 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:1808] 2023-05-31 03:56:43,005 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:1808] 2023-05-31 03:56:43,005 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1808] 2023-05-31 03:56:43,005 >> loading file tokenizer_config.json\n","05/31/2023 03:56:43 - INFO - __main__ - training files: /content/datas/ft.json\n","05/31/2023 03:56:43 - WARNING - root - building dataset...\n","05/31/2023 03:56:43 - INFO - datasets.builder - Using custom data configuration default-ac20cd1fd50e0ccb\n","05/31/2023 03:56:43 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","05/31/2023 03:56:43 - INFO - datasets.builder - Generating dataset json (/content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n","Downloading and preparing dataset json/default to /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n","Downloading data files: 100% 1/1 [00:00<00:00, 6887.20it/s]\n","05/31/2023 03:56:43 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","05/31/2023 03:56:43 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 1/1 [00:00<00:00, 1364.89it/s]\n","05/31/2023 03:56:43 - INFO - datasets.builder - Generating train split\n","05/31/2023 03:56:43 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n","100% 1/1 [00:00<00:00, 231.95it/s]\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #0 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00000_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #1 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00001_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #2 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00002_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #3 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00003_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #4 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00004_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #5 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00005_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #6 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00006_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #7 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00007_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Spawning 8 processes\n","preprocessing on dataset (num_proc=8):   0% 0/45 [00:00<?, ? examples/s]05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00005_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00002_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00001_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00004_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00006_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00003_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00007_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00000_of_00008.arrow\n","05/31/2023 03:56:44 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n","05/31/2023 03:56:44 - INFO - __main__ - Num train_samples  45\n","05/31/2023 03:56:44 - INFO - __main__ - training example:\n","05/31/2023 03:56:44 - INFO - __main__ - <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","ç³»ç»Ÿæ“ä½œå‘˜ï¼Œç”¨æˆ·è¯´ï¼š\n","æˆ‘æƒ³æŸ¥è¯¢æˆ‘çš„ç¤¾ä¿è®°å½•\n","\n","### Response:  response:\n","è¯·ç¨åï¼Œæ­£åœ¨å¯¹æ¥ç›¸å…³æœåŠ¡\n","command:\n","search(\"ç¤¾ä¿æŸ¥è¯¢æ¥å£\")</s>\n","05/31/2023 03:56:44 - INFO - __main__ - training files: /content/datas/ft.json\n","05/31/2023 03:56:44 - WARNING - root - building dataset...\n","05/31/2023 03:56:44 - INFO - __name__ - training datasets-/content/datas/ft.json has been loaded from disk\n","05/31/2023 03:56:44 - INFO - __main__ - Num eval_samples  45\n","05/31/2023 03:56:44 - INFO - __main__ - eval example:\n","05/31/2023 03:56:44 - INFO - __main__ - <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","ç³»ç»Ÿæ“ä½œå‘˜ï¼Œç”¨æˆ·è¯´ï¼š\n","æˆ‘æƒ³æŸ¥è¯¢æˆ‘çš„ç¤¾ä¿è®°å½•\n","\n","### Response:  response:\n","è¯·ç¨åï¼Œæ­£åœ¨å¯¹æ¥ç›¸å…³æœåŠ¡\n","command:\n","search(\"ç¤¾ä¿æŸ¥è¯¢æ¥å£\")</s>\n","[INFO|modeling_utils.py:2513] 2023-05-31 03:56:44,060 >> loading weights file /content/alpaca-combined/pytorch_model.bin.index.json\n","[INFO|modeling_utils.py:1154] 2023-05-31 03:56:44,061 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:577] 2023-05-31 03:56:44,062 >> Generate config GenerationConfig {\n","  \"_from_model_config\": true,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": -1,\n","  \"transformers_version\": \"4.29.2\"\n","}\n","\n","Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.83s/it]\n","[INFO|modeling_utils.py:3185] 2023-05-31 03:56:51,989 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:3193] 2023-05-31 03:56:51,989 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /content/alpaca-combined.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:537] 2023-05-31 03:56:51,992 >> loading configuration file /content/alpaca-combined/generation_config.json\n","[INFO|configuration_utils.py:577] 2023-05-31 03:56:51,992 >> Generate config GenerationConfig {\n","  \"_from_model_config\": true,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0,\n","  \"transformers_version\": \"4.29.2\"\n","}\n","\n","05/31/2023 03:56:51 - INFO - __main__ - len(tokenizer):49954\n","05/31/2023 03:56:51 - INFO - __main__ - Init new peft model\n","05/31/2023 03:56:51 - INFO - __main__ - target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n","05/31/2023 03:56:51 - INFO - __main__ - lora_rank: 8\n","trainable params: 838434816 || all params: 7314706432 || trainable%: 11.462316687544133\n","05/31/2023 03:58:24 - INFO - __main__ - model.modules_to_save: {'lm_head', 'embed_tokens'}\n","[INFO|trainer.py:565] 2023-05-31 03:58:24,996 >> max_steps is given, it will override any value given in num_train_epochs\n","[INFO|trainer.py:622] 2023-05-31 03:58:24,996 >> Using cuda_amp half precision backend\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[2023-05-31 03:58:25,029] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown\n","[2023-05-31 03:58:25,029] [INFO] [comm.py:606:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n","[2023-05-31 03:58:25,803] [INFO] [comm.py:656:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.28.0.12, master_port=29500\n","[2023-05-31 03:58:25,804] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n","05/31/2023 03:58:25 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0\n","05/31/2023 03:58:25 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n","05/31/2023 03:58:29 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\n","05/31/2023 03:58:29 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n","[2023-05-31 03:58:29,790] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n","[2023-05-31 03:58:29,790] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n","[2023-05-31 03:58:29,790] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n","[2023-05-31 03:58:29,836] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n","[2023-05-31 03:58:29,837] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>\n","[2023-05-31 03:58:29,837] [WARNING] [engine.py:1104:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","[2023-05-31 03:58:29,837] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n","[2023-05-31 03:58:29,837] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 100000000\n","[2023-05-31 03:58:29,837] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 100000000\n","[2023-05-31 03:58:29,837] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False\n","[2023-05-31 03:58:29,837] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False\n","Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n","Creating extension directory /root/.cache/torch_extensions/py310_cu118/utils...\n","Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/utils/build.ninja...\n","Building extension module utils...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n","[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n","Loading extension module utils...\n","Time to load utils op: 18.523107528686523 seconds\n","Rank: 0 partition count [1] and sizes[(838434816, False)] \n","[2023-05-31 03:58:51,511] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\n","[2023-05-31 03:58:51,512] [INFO] [utils.py:786:see_memory_usage] MA 16.78 GB         Max_MA 18.34 GB         CA 18.38 GB         Max_CA 18 GB \n","[2023-05-31 03:58:51,512] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.65 GB, percent = 5.6%\n","[2023-05-31 03:58:51,757] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\n","[2023-05-31 03:58:51,758] [INFO] [utils.py:786:see_memory_usage] MA 23.03 GB         Max_MA 29.27 GB         CA 30.88 GB         Max_CA 31 GB \n","[2023-05-31 03:58:51,758] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.65 GB, percent = 5.6%\n","[2023-05-31 03:58:51,759] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized\n","[2023-05-31 03:58:51,981] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\n","[2023-05-31 03:58:51,982] [INFO] [utils.py:786:see_memory_usage] MA 23.03 GB         Max_MA 23.03 GB         CA 30.88 GB         Max_CA 31 GB \n","[2023-05-31 03:58:51,982] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.65 GB, percent = 5.6%\n","[2023-05-31 03:58:51,996] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n","[2023-05-31 03:58:51,996] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n","[2023-05-31 03:58:51,996] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7fd9bdae52a0>\n","[2023-05-31 03:58:51,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n","[2023-05-31 03:58:52,001] [INFO] [config.py:955:print] DeepSpeedEngine configuration:\n","[2023-05-31 03:58:52,001] [INFO] [config.py:959:print]   activation_checkpointing_config  {\n","    \"partition_activations\": false, \n","    \"contiguous_memory_optimization\": false, \n","    \"cpu_checkpointing\": false, \n","    \"number_checkpoints\": null, \n","    \"synchronize_checkpoint_boundary\": false, \n","    \"profile\": false\n","}\n","[2023-05-31 03:58:52,001] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n","[2023-05-31 03:58:52,001] [INFO] [config.py:959:print]   amp_enabled .................. False\n","[2023-05-31 03:58:52,001] [INFO] [config.py:959:print]   amp_params ................... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   autotuning_config ............ {\n","    \"enabled\": false, \n","    \"start_step\": null, \n","    \"end_step\": null, \n","    \"metric_path\": null, \n","    \"arg_mappings\": null, \n","    \"metric\": \"throughput\", \n","    \"model_info\": null, \n","    \"results_dir\": \"autotuning_results\", \n","    \"exps_dir\": \"autotuning_exps\", \n","    \"overwrite\": true, \n","    \"fast\": true, \n","    \"start_profile_step\": 3, \n","    \"end_profile_step\": 5, \n","    \"tuner_type\": \"gridsearch\", \n","    \"tuner_early_stopping\": 5, \n","    \"tuner_num_trials\": 50, \n","    \"model_info_path\": null, \n","    \"mp_size\": 1, \n","    \"max_train_batch_size\": null, \n","    \"min_train_batch_size\": 1, \n","    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n","    \"min_train_micro_batch_size_per_gpu\": 1, \n","    \"num_tuning_micro_batch_sizes\": 3\n","}\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   bfloat16_enabled ............. False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd9bd970850>\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   communication_data_type ...... None\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   dataloader_drop_last ......... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   disable_allgather ............ False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   dump_state ................... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'min_scale': 1e-10}\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   elasticity_enabled ........... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   flops_profiler_config ........ {\n","    \"enabled\": false, \n","    \"profile_step\": 1, \n","    \"module_depth\": -1, \n","    \"top_modules\": 1, \n","    \"detailed\": true, \n","    \"output_file\": null\n","}\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   fp16_auto_cast ............... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   fp16_enabled ................. True\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   global_rank .................. 0\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   grad_accum_dtype ............. None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   load_universal_checkpoint .... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   loss_scale ................... 0\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   memory_breakdown ............. False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   mics_shard_size .............. -1\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   nebula_config ................ {\n","    \"enabled\": false, \n","    \"persistent_storage_path\": null, \n","    \"persistent_time_interval\": 100, \n","    \"num_of_version_in_retention\": 2, \n","    \"enable_nebula_load\": true, \n","    \"load_path\": null\n","}\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   optimizer_name ............... None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   optimizer_params ............. None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   pld_enabled .................. False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   pld_params ................... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   prescale_gradients ........... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   scheduler_name ............... None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   scheduler_params ............. None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   sparse_attention ............. None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   steps_per_print .............. 2000\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   train_batch_size ............. 1\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  1\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   use_node_local_storage ....... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   world_size ................... 1\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  True\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100000000 allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   zero_enabled ................. True\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   zero_optimization_stage ...... 2\n","[2023-05-31 03:58:52,004] [INFO] [config.py:945:print_user_config]   json = {\n","    \"fp16\": {\n","        \"enabled\": true, \n","        \"loss_scale\": 0, \n","        \"loss_scale_window\": 100, \n","        \"initial_scale_power\": 16, \n","        \"hysteresis\": 2, \n","        \"min_loss_scale\": 1e-10\n","    }, \n","    \"zero_optimization\": {\n","        \"stage\": 2, \n","        \"allgather_partitions\": true, \n","        \"allgather_bucket_size\": 1.000000e+08, \n","        \"overlap_comm\": true, \n","        \"reduce_scatter\": true, \n","        \"reduce_bucket_size\": 1.000000e+08, \n","        \"contiguous_gradients\": true\n","    }, \n","    \"gradient_accumulation_steps\": 1, \n","    \"gradient_clipping\": 1.0, \n","    \"steps_per_print\": 2.000000e+03, \n","    \"train_batch_size\": 1, \n","    \"train_micro_batch_size_per_gpu\": 1, \n","    \"wall_clock_breakdown\": false, \n","    \"zero_allow_untested_optimizer\": true\n","}\n","Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n","No modifications detected for re-loaded extension module utils, skipping build step...\n","Loading extension module utils...\n","Time to load utils op: 0.00036907196044921875 seconds\n","[INFO|trainer.py:1779] 2023-05-31 03:58:52,008 >> ***** Running training *****\n","[INFO|trainer.py:1780] 2023-05-31 03:58:52,008 >>   Num examples = 45\n","[INFO|trainer.py:1781] 2023-05-31 03:58:52,008 >>   Num Epochs = 3\n","[INFO|trainer.py:1782] 2023-05-31 03:58:52,008 >>   Instantaneous batch size per device = 1\n","[INFO|trainer.py:1783] 2023-05-31 03:58:52,008 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n","[INFO|trainer.py:1784] 2023-05-31 03:58:52,008 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1785] 2023-05-31 03:58:52,008 >>   Total optimization steps = 100\n","[INFO|trainer.py:1786] 2023-05-31 03:58:52,014 >>   Number of trainable parameters = 838,434,816\n","  0% 0/100 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-05-31 03:58:52,061 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","[2023-05-31 03:58:55,514] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n","{'loss': 3.6855, 'learning_rate': 0.0, 'epoch': 0.02}\n","  1% 1/100 [00:03<05:44,  3.48s/it][2023-05-31 03:58:55,871] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n","{'loss': 3.1781, 'learning_rate': 9.934583543669453e-05, 'epoch': 0.22}\n","{'loss': 1.1039, 'learning_rate': 9.421477453650118e-05, 'epoch': 0.44}\n","{'loss': 0.7322, 'learning_rate': 8.448618886390522e-05, 'epoch': 0.67}\n"," 31% 31/100 [00:17<00:33,  2.08it/s][2023-05-31 03:59:10,316] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n","{'loss': 0.4376, 'learning_rate': 7.262735145222696e-05, 'epoch': 0.89}\n","{'loss': 0.3977, 'learning_rate': 5.726142856227452e-05, 'epoch': 1.11}\n","{'loss': 0.2588, 'learning_rate': 4.114045042103887e-05, 'epoch': 1.33}\n","{'loss': 0.3199, 'learning_rate': 2.5940702775459747e-05, 'epoch': 1.56}\n","{'loss': 0.1503, 'learning_rate': 1.3242680314639993e-05, 'epoch': 1.78}\n","{'loss': 0.2083, 'learning_rate': 4.366744239922998e-06, 'epoch': 2.0}\n","{'loss': 0.051, 'learning_rate': 2.3582894166930268e-07, 'epoch': 2.22}\n","100% 100/100 [00:51<00:00,  2.04it/s][INFO|trainer.py:2052] 2023-05-31 03:59:43,235 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 51.2217, 'train_samples_per_second': 1.952, 'train_steps_per_second': 1.952, 'train_loss': 0.6888565826416015, 'epoch': 2.22}\n","100% 100/100 [00:51<00:00,  1.95it/s]\n","[INFO|trainer.py:2904] 2023-05-31 03:59:43,237 >> Saving model checkpoint to /content/chinese_sfted\n","[INFO|trainer.py:2916] 2023-05-31 03:59:43,248 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","[INFO|tokenization_utils_base.py:2181] 2023-05-31 03:59:46,141 >> tokenizer config file saved in /content/chinese_sfted/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2188] 2023-05-31 03:59:46,141 >> Special tokens file saved in /content/chinese_sfted/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =       2.22\n","  train_loss               =     0.6889\n","  train_runtime            = 0:00:51.22\n","  train_samples            =         45\n","  train_samples_per_second =      1.952\n","  train_steps_per_second   =      1.952\n","[INFO|tokenization_utils_base.py:2181] 2023-05-31 03:59:52,608 >> tokenizer config file saved in /content/chinese_sfted/sft_lora_model/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2188] 2023-05-31 03:59:52,609 >> Special tokens file saved in /content/chinese_sfted/sft_lora_model/special_tokens_map.json\n","05/31/2023 03:59:52 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:3165] 2023-05-31 03:59:52,611 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3167] 2023-05-31 03:59:52,611 >>   Num examples = 45\n","[INFO|trainer.py:3170] 2023-05-31 03:59:52,611 >>   Batch size = 1\n","100% 45/45 [00:03<00:00, 14.29it/s]\n","***** eval metrics *****\n","  epoch                   =       2.22\n","  eval_loss               =     0.1355\n","  eval_runtime            = 0:00:03.22\n","  eval_samples            =         45\n","  eval_samples_per_second =     13.942\n","  eval_steps_per_second   =     13.942\n","  perplexity              =     1.1451\n"]}]},{"cell_type":"markdown","source":["# æ”¶å°¾\n","\n","ç”Ÿæˆæ¨¡å‹æ”¾åˆ°äº†chinese_sftedæ–‡ä»¶å¤¹é‡Œé¢\n","\n","å°†ç”Ÿæˆçš„æ¨¡å‹å†æ¬¡æ‹·è´å›Google Driveï¼Œåç»­å°±å¯ä»¥è¿›è¡Œä¸‹è½½äº†ã€‚è¿™ä¸ªç”Ÿæˆæ¨¡å‹æ˜¯Loraæƒé‡ï¼Œå®é™…ä½¿ç”¨çš„æ—¶å€™æœ‰ä¸¤ç§åŠæ³•ï¼š\n","\n","* æ¨¡å‹åŠ è½½æ—¶ï¼ŒåŠ è½½åŸåˆå¹¶æ¨¡å‹ï¼Œä½†å¦å¤–æŒ‡å®š--loraå‚æ•°åŠ è½½loraæƒé‡ï¼Œè¿™ç§åŠæ³•è¾ƒæ…¢ï¼Œä½†å¯ä»¥ç”¨äºè¿‡ç¨‹ä¸­å¯¹ç²¾è°ƒæ¨¡å‹è¿›è¡ŒéªŒè¯\n","* å’ŒåŸåˆå¹¶æ¨¡å‹å†æ¬¡åˆå¹¶ï¼Œåˆå¹¶åä½œä¸ºå®Œæ•´æ¨¡å‹ä½¿ç”¨ã€‚\n","\n","ä¸ç®¡æ˜¯ä¸Šè¯‰å“ªç§ä½¿ç”¨åŠæ³•ï¼Œå»ºè®®éƒ½æ˜¯å°†loraæ¨¡å‹æ‹·è´å›æœ¬åœ°è¿›è¡Œåˆå¹¶ï¼Œä¸€æ˜¯å› ä¸ºåˆå¹¶çš„è®¾å¤‡è¦æ±‚ä¸é«˜ï¼Œåœ¨æœ¬åœ°å°±å¯ä»¥å®Œæˆï¼Œå¦å¤–æ˜¯å› ä¸ºloraæ¨¡å‹çš„ä½“é‡è¾ƒå®Œæ•´æ¨¡å‹å°ï¼Œæˆ‘è¿™é‡Œ7Bç‰ˆæœ¬çš„åŸå§‹åˆå¹¶æ¨¡å‹åœ¨16Gå·¦å³ï¼Œç²¾è°ƒæƒé‡å¤§æ¦‚1.5Gï¼Œå·®å¾ˆå¤šï¼Œè¿™æ ·ï¼Œåœ¨å›½å†…ç¯å¢ƒä¸‹ï¼Œä¸‹è½½ç²¾è°ƒæ¨¡å‹æ›´å®¹æ˜“ä¸€äº›ã€‚"],"metadata":{"id":"bd9HDO1hiuUH"}},{"cell_type":"markdown","source":["*é¢„å…ˆå¯¹è¾“å‡ºæ–‡ä»¶åè¿›è¡Œæ›´æ”¹ï¼ŒåŸå› è§ä¸‹ä¸€æ®µï¼Œè¿™é‡Œé¢„å…ˆæ‰§è¡Œä¸»è¦æ˜¯å› ä¸ºå¦‚æœç§»åŠ¨åæ”¹åçš„è¯ï¼Œgoogle driveä¼šæŠŠmvå‰åçš„æ–‡ä»¶éƒ½è‡ªåŠ¨ä¸‹è½½ï¼Œé€ æˆèµ„æºçš„æµªè´¹*\n","\n","\n","***deperatedï¼š***æœ€æ–°ç‰ˆChinese-LLaMA-Alpacaå·²ç»è‡ªåŠ¨å®Œæˆäº†æ ¼å¼è½¬æ¢ï¼Œè¯¥æ­¥éª¤å·²ä¸éœ€è¦æ‰§è¡Œ\n","\n","\n","---"],"metadata":{"id":"kVD8FDkjYX7S"}},{"cell_type":"code","source":["!mv /content/chinese_sfted/pytorch_model.bin /content/chinese_sfted/adapter_model.bin"],"metadata":{"id":"uwOd7bhZwQAi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ä»¥ä¸‹æ‹·è´ç›®å½•å’Œæ‹·è´zipåŒ…æ“ä½œ2é€‰1\n","\n","é€‰é¡¹1ï¼šæ‹·è´ç»“æœç›®å½•"],"metadata":{"id":"75Xa1dCJvX4m"}},{"cell_type":"code","source":["!cp -fr /content/chinese_sfted/sft_lora_model /content/drive/MyDrive/"],"metadata":{"id":"oGZMnCMEc-aE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["é€‰é¡¹2ï¼š\n","å¯ä»¥ç›´æ¥æ‰“åŒ…åå†æ‹·è´åˆ°google drive,æ–¹ä¾¿ä¹‹åçš„ä¸‹è½½ç­‰æ“ä½œ"],"metadata":{"id":"LK8Z2pKovMTQ"}},{"cell_type":"code","source":["!zip -j -r /content/chinese_sfted.zip /content/chinese_sfted/sft_lora_model"],"metadata":{"id":"h-GVcFpuus7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -fr /content/chinese_sfted.zip /content/drive/MyDrive/"],"metadata":{"id":"kOR3Z0g5-dDe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# å…³äºäº§å‡ºç‰©\n","\n","***deperatedï¼š***æœ€æ–°ç‰ˆChinese-LLaMA-Alpacaå·²ç»è‡ªåŠ¨å®Œæˆäº†æ ¼å¼è½¬æ¢ï¼Œè¯¥æ­¥éª¤å·²ä¸éœ€è¦æ‰§è¡Œ\n","\n","\n","---\n","\n","\n","äº§å‡ºç‰©ä¸ºLoraæƒé‡ï¼Œç›®å½•ç»“æ„ä¸­åŒ…å«pytorch_model.binæ–‡ä»¶ï¼Œä½†å¦‚æœç”¨äºåˆå¹¶ï¼Œå®é™…éœ€è¦ä¸¤ä¸ªé¢å¤–æ–‡ä»¶\n","\n","* adapter_model.bin (å°†pytorch_model.biné‡æ–°å‘½å)\n","* adapter_config.json (è¿™ä¸ªé…ç½®æ–‡ä»¶çš„ä¸»ä½“å†…å®¹å¦‚ä¸‹ï¼Œå…¶ä¸­å‚æ•°å’Œæˆ‘ä»¬æ‰§è¡Œä¸Šè¿°ç²¾è°ƒå‘½ä»¤æ—¶çš„å‚æ•°æœ‰å¯¹åº”å…³ç³»æ‰§è¡Œå¯ä»¥ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼‰\n","\n","å¦‚æœä½ è°ƒæ•´äº†å‚æ•°ï¼Œè¯·å¯¹åº”è°ƒæ•´è¿™é‡Œçš„å‚æ•°"],"metadata":{"id":"yDXatW0Ao4C5"}},{"cell_type":"code","source":["f = open(\"/content/drive/MyDrive/chinese_sfted/adapter_confjg.json\", \"a\")\n","f.write(\"\"\"{\n","  \"base_model_name_or_path\": \"/content/alpaca-combined\"\n","  \"bias\": \"none\",\n","  \"enable_lora\": null,\n","  \"fan_in_fan_out\": false,\n","  \"inference_mode\": true,\n","  \"init_lora_weights\": true,\n","  \"lora_alpha\": 32,\n","  \"lora_dropout\": 0.05,\n","  \"merge_weights\": false,\n","  \"modules_to_save\": [\n","    \"embed_tokens\",\n","    \"lm_head\"\n","  ],\n","  \"peft_type\": \"LORA\",\n","  \"r\": 8,\n","  \"target_modules\": [\n","    \"q_proj\",\n","    \"v_proj\",\n","    \"k_proj\",\n","    \"o_proj\",\n","    \"gate_proj\",\n","    \"down_proj\",\n","    \"up_proj\"\n","  ],\n","  \"task_type\":\"CAUSAL_LM\"\n","}\"\"\")\n","f.close()"],"metadata":{"id":"laDcRe3V0ObI"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1fKpMX7i6KognPjqmYE6mOqrS3zwE1SEE","timestamp":1685061346926},{"file_id":"1DgBrWVyTaxeRdPkToSAattDHhLGPKl23","timestamp":1684138213170},{"file_id":"1axIgPoThgm-v3rglmRV9QnhVsJKHsHBj","timestamp":1684120777048},{"file_id":"1Eak6azD3MLeb-YsfbP8UZC8wrL1ddIMI","timestamp":1682666840625}],"gpuType":"A100","gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}