{"cells":[{"cell_type":"markdown","metadata":{"id":"B1c96_k3MahN"},"source":["# 转换并量化中文Alpaca Plus模型\n","\n","这是和知乎系列精调文章\n","\n","* [尝试对Chinese-LLaMA-Alpaca进行微调-准备篇](https://zhuanlan.zhihu.com/p/630522733)\n","* [尝试对Chinese-LLaMA-Alpaca进行微调-数据准备篇](https://zhuanlan.zhihu.com/p/630544641)\n","* [尝试对Chinese-LLaMA-Alpaca进行微调-实践篇](https://zhuanlan.zhihu.com/p/632291297)\n","\n","\n","对应的在线脚本。\n","\n","和官方脚本相比增加了对于模型的存储过程，以便我们可以在后续的步骤中进行进一步的精调。\n","\n","官方脚本中有很多关于Google Colab花费，经验等的介绍，大家可以多多查阅\n","\n","[官方脚本](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/notebooks/convert_and_quantize_chinese_alpaca_plus.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"vScqHD_jMFOV"},"source":["## 安装相关依赖\n","\n","和官方不同的是这个地方会多安装一个mpi4py，用于精调阶段"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5WKFJXIL6ZU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685505190896,"user_tz":-480,"elapsed":99043,"user":{"displayName":"刘铭","userId":"16932740466479782785"}},"outputId":"9dbe5bfb-9c6d-42ed-f20a-aa029c647cac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/peft\n","  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-ad6af76y\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-ad6af76y\n","  Resolved https://github.com/huggingface/peft to commit 3714aa2fff158fdfa637b2b65952580801d890b2\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (6.0)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (2.0.1+cu118)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (4.29.2)\n","Collecting accelerate (from peft==0.4.0.dev0)\n","  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (16.0.5)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (0.14.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0.dev0) (2023.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.4.0.dev0) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.4.0.dev0) (1.3.0)\n","Building wheels for collected packages: peft\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=56971 sha256=e7bc42160d8d1a0036d4c770facb8473b0cf65b56faf247f4df94c3bda6419a3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-hly4p0cv/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\n","Successfully built peft\n","Installing collected packages: accelerate, peft\n","Successfully installed accelerate-0.19.0 peft-0.4.0.dev0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mpi4py\n","  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: mpi4py\n","  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp310-cp310-linux_x86_64.whl size=3365632 sha256=aa88fcf7382ac28fc7dc73dca8f4c0ac35a405cebd55f128847b47b66d844eb1\n","  Stored in directory: /root/.cache/pip/wheels/e8/1b/b5/97ec4cfccdde26e0f3590ad6e09a5242d508dff09704ef86c1\n","Successfully built mpi4py\n","Installing collected packages: mpi4py\n","Successfully installed mpi4py-3.1.4\n"]}],"source":["!pip install torch\n","!pip install transformers\n","!pip install git+https://github.com/huggingface/peft\n","!pip install sentencepiece\n","!pip install mpi4py"]},{"cell_type":"markdown","metadata":{"id":"ygb1xFIMNQKw"},"source":["## 克隆目录和代码"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCEJh7NJNXz9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685505223718,"user_tz":-480,"elapsed":1926,"user":{"displayName":"刘铭","userId":"16932740466479782785"}},"outputId":"239e9129-e6b5-438f-fabd-1b6def31da4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Chinese-LLaMA-Alpaca'...\n","remote: Enumerating objects: 1153, done.\u001b[K\n","remote: Counting objects: 100% (345/345), done.\u001b[K\n","remote: Compressing objects: 100% (149/149), done.\u001b[K\n","remote: Total 1153 (delta 216), reused 266 (delta 193), pack-reused 808\u001b[K\n","Receiving objects: 100% (1153/1153), 18.51 MiB | 13.65 MiB/s, done.\n","Resolving deltas: 100% (678/678), done.\n"]}],"source":["!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca"]},{"cell_type":"markdown","source":["# 完成Google Drive的加载"],"metadata":{"id":"2QFYReeYgq2D"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUbLnPr3b3A-","executionInfo":{"status":"ok","timestamp":1685505221796,"user_tz":-480,"elapsed":30908,"user":{"displayName":"刘铭","userId":"16932740466479782785"}},"outputId":"d87a29d1-c6d8-4c91-f04e-f37b3bb708de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"nIyxX0DSNsgQ"},"source":["# 合并模型（Alpaca-Plus-7B）\n","\n","## 注意：本步骤只执行一次\n","\n","本步骤是对模型进行合并，请注意，这个步骤在我们整体精调步骤中只会执行一次，从第二次开始，都直接跳过这个步骤，直接从Google Drive中加载模型\n","\n","## 提醒：除精调阶段外，其他步骤可选用花费较少的机型\n","\n","如果你已经了解到整个流程其实分成了第一次合并，获得中文合并模型，和之后的精调阶段，那么你第一次执行的时候就可以不用选择太贵的硬件，合并不需要太多GPU资源\n","\n","## 官方提示\n","\n","💡 转换13B模型提示：\n","- 请将参数`--base_model`和`--lora_model`中的的`7b`改为`13b`即可\n","- **免费用户必须增加一个参数`--offload_dir`以缓解内存压力**，例如`--offload_dir ./offload_temp`\n","\n","该过程比较耗时（下载+转换），需要几分钟到十几分钟不等，请耐心等待。\n","转换好的模型存放在`alpaca-combined`目录。\n","如果你不需要量化模型，那么到这一步就结束了。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AV4EW5hNhVV"},"outputs":[],"source":["!python ./Chinese-LLaMA-Alpaca/scripts/merge_llama_with_chinese_lora.py \\\n","    --base_model decapoda-research/llama-7b-hf \\\n","    --lora_model ziqingyang/chinese-llama-plus-lora-7b,ziqingyang/chinese-alpaca-plus-lora-7b \\\n","    --output_type huggingface \\\n","    --output_dir alpaca-combined"]},{"cell_type":"markdown","source":["## 保存合并好的模型\n","\n","这个步骤将合并好的模型拷贝保存到Google Drive中，以便在以后的精调工作中使用，请注意，模型比较大，需要收费版本才能存下。\n","\n","为了以后和精调权重再次合并，这个合并模型请从Google Drive中下载一份到本地，以后就可以在本地合并最终模型了，具体原因在文末。"],"metadata":{"id":"r814pb-6g0u9"}},{"cell_type":"code","source":["!cp -fr /content/alpaca-combined /content/drive/MyDrive/"],"metadata":{"id":"BkKXEDldhIUL","executionInfo":{"status":"ok","timestamp":1685494405725,"user_tz":-480,"elapsed":6,"user":{"displayName":"刘铭","userId":"16932740466479782785"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d3b7d391-0cf9-4ff5-f7ac-0c7eccf73249"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/alpaca-combined': No such file or directory\n"]}]},{"cell_type":"markdown","source":["# 直接从driver load\n","\n","## 注意，除第一次外都执行以下步骤"],"metadata":{"id":"fn7f1x0Xbun7"}},{"cell_type":"markdown","source":["拷贝模型文件"],"metadata":{"id":"JDFgVLV6b5Gv"}},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/alpaca-combined /content/"],"metadata":{"id":"T9oVB8l_b9EM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["拷贝精调文件\n","\n","注意：精调文件这里假设是ft.json，可以根据自己的文件名，调整第一个ft.json，目标名称请不要改动，以便后续步骤中对齐"],"metadata":{"id":"rOMweaUjhT0G"}},{"cell_type":"code","source":["!mkdir -p datas\n","!cp /content/drive/MyDrive/ft.json /content/datas/ft.json"],"metadata":{"id":"jJFAYFNLcneA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLkuRAo9Vkb1"},"source":["## 进行进一步精调\n"]},{"cell_type":"code","source":["!pip install datasets\n","!pip install deepspeed"],"metadata":{"id":"_WmxcC255Rtv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685505388835,"user_tz":-480,"elapsed":30711,"user":{"displayName":"刘铭","userId":"16932740466479782785"}},"outputId":"3201f87e-5a1b-465c-84a1-ef09c4886753"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Collecting responses<0.19 (from datasets)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting deepspeed\n","  Downloading deepspeed-0.9.2.tar.gz (779 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.3/779.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting hjson (from deepspeed)\n","  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja (from deepspeed)\n","  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n","Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.10.7)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.0.1+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.65.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0->deepspeed) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.12.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->deepspeed) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->deepspeed) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n","Building wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.9.2-py3-none-any.whl size=811220 sha256=aec7458a622095bd2c8161947d3dd3042154df0ec8af24564a7c7138e3eb804e\n","  Stored in directory: /root/.cache/pip/wheels/a6/d2/b1/b15210b5dc024bab4eccbac2148db29959fe01fe6042557d07\n","Successfully built deepspeed\n","Installing collected packages: ninja, hjson, deepspeed\n","Successfully installed deepspeed-0.9.2 hjson-3.1.0 ninja-1.11.1\n"]}]},{"cell_type":"markdown","source":["执行前，请完成以下设置：\n","\n","* 在菜单：代码执行程序中点击更改运行时类型\n","* 在硬件加速器中选择GPU\n","* 在GPU类型中选择合适的类型，我一般使用A100，其他型号似乎内存不足。\n","\n","在我的使用过程中，一次精调大概20分钟左右，花费点数5-6个点，100个点9.99，一次大概合人民币3，4块钱吧，当然这个取决于你的数据量大小"],"metadata":{"id":"vTYm60AcfQaT"}},{"cell_type":"code","source":["!cd ./Chinese-LLaMA-Alpaca/scripts && python run_clm_sft_with_peft.py --deepspeed ds_zero2_no_offload.json --model_name_or_path /content/alpaca-combined --tokenizer_name_or_path /content/alpaca-combined --dataset_dir /content/datas --validation_split_percentage 0.001 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --do_train --do_eval --seed $RANDOM --max_steps 100 --lr_scheduler_type cosine --learning_rate 1e-4 --warmup_ratio 0.03 --weight_decay 0 --logging_strategy steps --logging_steps 10 --save_strategy steps --save_total_limit 3 --evaluation_strategy steps --eval_steps 250 --save_steps 500 --gradient_accumulation_steps 1 --preprocessing_num_workers 8 --max_seq_length 512 --output_dir /content/chinese_sfted --overwrite_output_dir --ddp_timeout 30000 --logging_first_step True --lora_rank 8 --lora_alpha 32 --trainable \"q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj\" --modules_to_save \"embed_tokens,lm_head\" --lora_dropout 0.05 --fp16 --torch_dtype float16 --validation_file /content/datas/ft.json --force_resize_embeddings False --gradient_checkpointing --ddp_find_unused_parameters False"],"metadata":{"id":"ZX3vAnJP7cf7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685505597022,"user_tz":-480,"elapsed":208196,"user":{"displayName":"刘铭","userId":"16932740466479782785"}},"outputId":"f091b16c-3626-4c47-c4f2-63f9e3f2ee74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-31 03:56:34.996177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","05/31/2023 03:56:43 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n","[INFO|configuration_utils.py:667] 2023-05-31 03:56:43,004 >> loading configuration file /content/alpaca-combined/config.json\n","[INFO|configuration_utils.py:725] 2023-05-31 03:56:43,004 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"/content/alpaca-combined\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 2048,\n","  \"max_sequence_length\": 2048,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"pad_token_id\": -1,\n","  \"rms_norm_eps\": 1e-06,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.29.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 49954\n","}\n","\n","[INFO|tokenization_utils_base.py:1808] 2023-05-31 03:56:43,005 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:1808] 2023-05-31 03:56:43,005 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:1808] 2023-05-31 03:56:43,005 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1808] 2023-05-31 03:56:43,005 >> loading file tokenizer_config.json\n","05/31/2023 03:56:43 - INFO - __main__ - training files: /content/datas/ft.json\n","05/31/2023 03:56:43 - WARNING - root - building dataset...\n","05/31/2023 03:56:43 - INFO - datasets.builder - Using custom data configuration default-ac20cd1fd50e0ccb\n","05/31/2023 03:56:43 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","05/31/2023 03:56:43 - INFO - datasets.builder - Generating dataset json (/content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n","Downloading and preparing dataset json/default to /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n","Downloading data files: 100% 1/1 [00:00<00:00, 6887.20it/s]\n","05/31/2023 03:56:43 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","05/31/2023 03:56:43 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 1/1 [00:00<00:00, 1364.89it/s]\n","05/31/2023 03:56:43 - INFO - datasets.builder - Generating train split\n","05/31/2023 03:56:43 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n","100% 1/1 [00:00<00:00, 231.95it/s]\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #0 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00000_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #1 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00001_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #2 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00002_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #3 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00003_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #4 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00004_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #5 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00005_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #6 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00006_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Process #7 will write at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00007_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Spawning 8 processes\n","preprocessing on dataset (num_proc=8):   0% 0/45 [00:00<?, ? examples/s]05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00005_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00002_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00001_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00004_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00006_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00003_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00007_of_00008.arrow\n","05/31/2023 03:56:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/datas/ft/json/default-ac20cd1fd50e0ccb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bb059991f87840fc_00000_of_00008.arrow\n","05/31/2023 03:56:44 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n","05/31/2023 03:56:44 - INFO - __main__ - Num train_samples  45\n","05/31/2023 03:56:44 - INFO - __main__ - training example:\n","05/31/2023 03:56:44 - INFO - __main__ - <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","系统操作员，用户说：\n","我想查询我的社保记录\n","\n","### Response:  response:\n","请稍后，正在对接相关服务\n","command:\n","search(\"社保查询接口\")</s>\n","05/31/2023 03:56:44 - INFO - __main__ - training files: /content/datas/ft.json\n","05/31/2023 03:56:44 - WARNING - root - building dataset...\n","05/31/2023 03:56:44 - INFO - __name__ - training datasets-/content/datas/ft.json has been loaded from disk\n","05/31/2023 03:56:44 - INFO - __main__ - Num eval_samples  45\n","05/31/2023 03:56:44 - INFO - __main__ - eval example:\n","05/31/2023 03:56:44 - INFO - __main__ - <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","系统操作员，用户说：\n","我想查询我的社保记录\n","\n","### Response:  response:\n","请稍后，正在对接相关服务\n","command:\n","search(\"社保查询接口\")</s>\n","[INFO|modeling_utils.py:2513] 2023-05-31 03:56:44,060 >> loading weights file /content/alpaca-combined/pytorch_model.bin.index.json\n","[INFO|modeling_utils.py:1154] 2023-05-31 03:56:44,061 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:577] 2023-05-31 03:56:44,062 >> Generate config GenerationConfig {\n","  \"_from_model_config\": true,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": -1,\n","  \"transformers_version\": \"4.29.2\"\n","}\n","\n","Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.83s/it]\n","[INFO|modeling_utils.py:3185] 2023-05-31 03:56:51,989 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:3193] 2023-05-31 03:56:51,989 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /content/alpaca-combined.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:537] 2023-05-31 03:56:51,992 >> loading configuration file /content/alpaca-combined/generation_config.json\n","[INFO|configuration_utils.py:577] 2023-05-31 03:56:51,992 >> Generate config GenerationConfig {\n","  \"_from_model_config\": true,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0,\n","  \"transformers_version\": \"4.29.2\"\n","}\n","\n","05/31/2023 03:56:51 - INFO - __main__ - len(tokenizer):49954\n","05/31/2023 03:56:51 - INFO - __main__ - Init new peft model\n","05/31/2023 03:56:51 - INFO - __main__ - target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n","05/31/2023 03:56:51 - INFO - __main__ - lora_rank: 8\n","trainable params: 838434816 || all params: 7314706432 || trainable%: 11.462316687544133\n","05/31/2023 03:58:24 - INFO - __main__ - model.modules_to_save: {'lm_head', 'embed_tokens'}\n","[INFO|trainer.py:565] 2023-05-31 03:58:24,996 >> max_steps is given, it will override any value given in num_train_epochs\n","[INFO|trainer.py:622] 2023-05-31 03:58:24,996 >> Using cuda_amp half precision backend\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[2023-05-31 03:58:25,029] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown\n","[2023-05-31 03:58:25,029] [INFO] [comm.py:606:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n","[2023-05-31 03:58:25,803] [INFO] [comm.py:656:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.28.0.12, master_port=29500\n","[2023-05-31 03:58:25,804] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n","05/31/2023 03:58:25 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0\n","05/31/2023 03:58:25 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n","05/31/2023 03:58:29 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\n","05/31/2023 03:58:29 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n","[2023-05-31 03:58:29,790] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n","[2023-05-31 03:58:29,790] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n","[2023-05-31 03:58:29,790] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n","[2023-05-31 03:58:29,836] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n","[2023-05-31 03:58:29,837] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>\n","[2023-05-31 03:58:29,837] [WARNING] [engine.py:1104:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","[2023-05-31 03:58:29,837] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n","[2023-05-31 03:58:29,837] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 100000000\n","[2023-05-31 03:58:29,837] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 100000000\n","[2023-05-31 03:58:29,837] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False\n","[2023-05-31 03:58:29,837] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False\n","Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n","Creating extension directory /root/.cache/torch_extensions/py310_cu118/utils...\n","Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/utils/build.ninja...\n","Building extension module utils...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n","[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n","Loading extension module utils...\n","Time to load utils op: 18.523107528686523 seconds\n","Rank: 0 partition count [1] and sizes[(838434816, False)] \n","[2023-05-31 03:58:51,511] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\n","[2023-05-31 03:58:51,512] [INFO] [utils.py:786:see_memory_usage] MA 16.78 GB         Max_MA 18.34 GB         CA 18.38 GB         Max_CA 18 GB \n","[2023-05-31 03:58:51,512] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.65 GB, percent = 5.6%\n","[2023-05-31 03:58:51,757] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\n","[2023-05-31 03:58:51,758] [INFO] [utils.py:786:see_memory_usage] MA 23.03 GB         Max_MA 29.27 GB         CA 30.88 GB         Max_CA 31 GB \n","[2023-05-31 03:58:51,758] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.65 GB, percent = 5.6%\n","[2023-05-31 03:58:51,759] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized\n","[2023-05-31 03:58:51,981] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\n","[2023-05-31 03:58:51,982] [INFO] [utils.py:786:see_memory_usage] MA 23.03 GB         Max_MA 23.03 GB         CA 30.88 GB         Max_CA 31 GB \n","[2023-05-31 03:58:51,982] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 4.65 GB, percent = 5.6%\n","[2023-05-31 03:58:51,996] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n","[2023-05-31 03:58:51,996] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n","[2023-05-31 03:58:51,996] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7fd9bdae52a0>\n","[2023-05-31 03:58:51,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n","[2023-05-31 03:58:52,001] [INFO] [config.py:955:print] DeepSpeedEngine configuration:\n","[2023-05-31 03:58:52,001] [INFO] [config.py:959:print]   activation_checkpointing_config  {\n","    \"partition_activations\": false, \n","    \"contiguous_memory_optimization\": false, \n","    \"cpu_checkpointing\": false, \n","    \"number_checkpoints\": null, \n","    \"synchronize_checkpoint_boundary\": false, \n","    \"profile\": false\n","}\n","[2023-05-31 03:58:52,001] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n","[2023-05-31 03:58:52,001] [INFO] [config.py:959:print]   amp_enabled .................. False\n","[2023-05-31 03:58:52,001] [INFO] [config.py:959:print]   amp_params ................... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   autotuning_config ............ {\n","    \"enabled\": false, \n","    \"start_step\": null, \n","    \"end_step\": null, \n","    \"metric_path\": null, \n","    \"arg_mappings\": null, \n","    \"metric\": \"throughput\", \n","    \"model_info\": null, \n","    \"results_dir\": \"autotuning_results\", \n","    \"exps_dir\": \"autotuning_exps\", \n","    \"overwrite\": true, \n","    \"fast\": true, \n","    \"start_profile_step\": 3, \n","    \"end_profile_step\": 5, \n","    \"tuner_type\": \"gridsearch\", \n","    \"tuner_early_stopping\": 5, \n","    \"tuner_num_trials\": 50, \n","    \"model_info_path\": null, \n","    \"mp_size\": 1, \n","    \"max_train_batch_size\": null, \n","    \"min_train_batch_size\": 1, \n","    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n","    \"min_train_micro_batch_size_per_gpu\": 1, \n","    \"num_tuning_micro_batch_sizes\": 3\n","}\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   bfloat16_enabled ............. False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd9bd970850>\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   communication_data_type ...... None\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   dataloader_drop_last ......... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   disable_allgather ............ False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   dump_state ................... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'min_scale': 1e-10}\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   elasticity_enabled ........... False\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   flops_profiler_config ........ {\n","    \"enabled\": false, \n","    \"profile_step\": 1, \n","    \"module_depth\": -1, \n","    \"top_modules\": 1, \n","    \"detailed\": true, \n","    \"output_file\": null\n","}\n","[2023-05-31 03:58:52,002] [INFO] [config.py:959:print]   fp16_auto_cast ............... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   fp16_enabled ................. True\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   global_rank .................. 0\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   grad_accum_dtype ............. None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   gradient_clipping ............ 1.0\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   load_universal_checkpoint .... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   loss_scale ................... 0\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   memory_breakdown ............. False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   mics_shard_size .............. -1\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   nebula_config ................ {\n","    \"enabled\": false, \n","    \"persistent_storage_path\": null, \n","    \"persistent_time_interval\": 100, \n","    \"num_of_version_in_retention\": 2, \n","    \"enable_nebula_load\": true, \n","    \"load_path\": null\n","}\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   optimizer_name ............... None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   optimizer_params ............. None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   pld_enabled .................. False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   pld_params ................... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   prescale_gradients ........... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   scheduler_name ............... None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   scheduler_params ............. None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   sparse_attention ............. None\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   steps_per_print .............. 2000\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   train_batch_size ............. 1\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  1\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   use_node_local_storage ....... False\n","[2023-05-31 03:58:52,003] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   world_size ................... 1\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  True\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100000000 allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   zero_enabled ................. True\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True\n","[2023-05-31 03:58:52,004] [INFO] [config.py:959:print]   zero_optimization_stage ...... 2\n","[2023-05-31 03:58:52,004] [INFO] [config.py:945:print_user_config]   json = {\n","    \"fp16\": {\n","        \"enabled\": true, \n","        \"loss_scale\": 0, \n","        \"loss_scale_window\": 100, \n","        \"initial_scale_power\": 16, \n","        \"hysteresis\": 2, \n","        \"min_loss_scale\": 1e-10\n","    }, \n","    \"zero_optimization\": {\n","        \"stage\": 2, \n","        \"allgather_partitions\": true, \n","        \"allgather_bucket_size\": 1.000000e+08, \n","        \"overlap_comm\": true, \n","        \"reduce_scatter\": true, \n","        \"reduce_bucket_size\": 1.000000e+08, \n","        \"contiguous_gradients\": true\n","    }, \n","    \"gradient_accumulation_steps\": 1, \n","    \"gradient_clipping\": 1.0, \n","    \"steps_per_print\": 2.000000e+03, \n","    \"train_batch_size\": 1, \n","    \"train_micro_batch_size_per_gpu\": 1, \n","    \"wall_clock_breakdown\": false, \n","    \"zero_allow_untested_optimizer\": true\n","}\n","Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n","No modifications detected for re-loaded extension module utils, skipping build step...\n","Loading extension module utils...\n","Time to load utils op: 0.00036907196044921875 seconds\n","[INFO|trainer.py:1779] 2023-05-31 03:58:52,008 >> ***** Running training *****\n","[INFO|trainer.py:1780] 2023-05-31 03:58:52,008 >>   Num examples = 45\n","[INFO|trainer.py:1781] 2023-05-31 03:58:52,008 >>   Num Epochs = 3\n","[INFO|trainer.py:1782] 2023-05-31 03:58:52,008 >>   Instantaneous batch size per device = 1\n","[INFO|trainer.py:1783] 2023-05-31 03:58:52,008 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n","[INFO|trainer.py:1784] 2023-05-31 03:58:52,008 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1785] 2023-05-31 03:58:52,008 >>   Total optimization steps = 100\n","[INFO|trainer.py:1786] 2023-05-31 03:58:52,014 >>   Number of trainable parameters = 838,434,816\n","  0% 0/100 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-05-31 03:58:52,061 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","[2023-05-31 03:58:55,514] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n","{'loss': 3.6855, 'learning_rate': 0.0, 'epoch': 0.02}\n","  1% 1/100 [00:03<05:44,  3.48s/it][2023-05-31 03:58:55,871] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n","{'loss': 3.1781, 'learning_rate': 9.934583543669453e-05, 'epoch': 0.22}\n","{'loss': 1.1039, 'learning_rate': 9.421477453650118e-05, 'epoch': 0.44}\n","{'loss': 0.7322, 'learning_rate': 8.448618886390522e-05, 'epoch': 0.67}\n"," 31% 31/100 [00:17<00:33,  2.08it/s][2023-05-31 03:59:10,316] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n","{'loss': 0.4376, 'learning_rate': 7.262735145222696e-05, 'epoch': 0.89}\n","{'loss': 0.3977, 'learning_rate': 5.726142856227452e-05, 'epoch': 1.11}\n","{'loss': 0.2588, 'learning_rate': 4.114045042103887e-05, 'epoch': 1.33}\n","{'loss': 0.3199, 'learning_rate': 2.5940702775459747e-05, 'epoch': 1.56}\n","{'loss': 0.1503, 'learning_rate': 1.3242680314639993e-05, 'epoch': 1.78}\n","{'loss': 0.2083, 'learning_rate': 4.366744239922998e-06, 'epoch': 2.0}\n","{'loss': 0.051, 'learning_rate': 2.3582894166930268e-07, 'epoch': 2.22}\n","100% 100/100 [00:51<00:00,  2.04it/s][INFO|trainer.py:2052] 2023-05-31 03:59:43,235 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 51.2217, 'train_samples_per_second': 1.952, 'train_steps_per_second': 1.952, 'train_loss': 0.6888565826416015, 'epoch': 2.22}\n","100% 100/100 [00:51<00:00,  1.95it/s]\n","[INFO|trainer.py:2904] 2023-05-31 03:59:43,237 >> Saving model checkpoint to /content/chinese_sfted\n","[INFO|trainer.py:2916] 2023-05-31 03:59:43,248 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n","[INFO|tokenization_utils_base.py:2181] 2023-05-31 03:59:46,141 >> tokenizer config file saved in /content/chinese_sfted/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2188] 2023-05-31 03:59:46,141 >> Special tokens file saved in /content/chinese_sfted/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =       2.22\n","  train_loss               =     0.6889\n","  train_runtime            = 0:00:51.22\n","  train_samples            =         45\n","  train_samples_per_second =      1.952\n","  train_steps_per_second   =      1.952\n","[INFO|tokenization_utils_base.py:2181] 2023-05-31 03:59:52,608 >> tokenizer config file saved in /content/chinese_sfted/sft_lora_model/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2188] 2023-05-31 03:59:52,609 >> Special tokens file saved in /content/chinese_sfted/sft_lora_model/special_tokens_map.json\n","05/31/2023 03:59:52 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:3165] 2023-05-31 03:59:52,611 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3167] 2023-05-31 03:59:52,611 >>   Num examples = 45\n","[INFO|trainer.py:3170] 2023-05-31 03:59:52,611 >>   Batch size = 1\n","100% 45/45 [00:03<00:00, 14.29it/s]\n","***** eval metrics *****\n","  epoch                   =       2.22\n","  eval_loss               =     0.1355\n","  eval_runtime            = 0:00:03.22\n","  eval_samples            =         45\n","  eval_samples_per_second =     13.942\n","  eval_steps_per_second   =     13.942\n","  perplexity              =     1.1451\n"]}]},{"cell_type":"markdown","source":["# 收尾\n","\n","生成模型放到了chinese_sfted文件夹里面\n","\n","将生成的模型再次拷贝回Google Drive，后续就可以进行下载了。这个生成模型是Lora权重，实际使用的时候有两种办法：\n","\n","* 模型加载时，加载原合并模型，但另外指定--lora参数加载lora权重，这种办法较慢，但可以用于过程中对精调模型进行验证\n","* 和原合并模型再次合并，合并后作为完整模型使用。\n","\n","不管是上诉哪种使用办法，建议都是将lora模型拷贝回本地进行合并，一是因为合并的设备要求不高，在本地就可以完成，另外是因为lora模型的体量较完整模型小，我这里7B版本的原始合并模型在16G左右，精调权重大概1.5G，差很多，这样，在国内环境下，下载精调模型更容易一些。"],"metadata":{"id":"bd9HDO1hiuUH"}},{"cell_type":"markdown","source":["*预先对输出文件名进行更改，原因见下一段，这里预先执行主要是因为如果移动后改名的话，google drive会把mv前后的文件都自动下载，造成资源的浪费*\n","\n","\n","***deperated：***最新版Chinese-LLaMA-Alpaca已经自动完成了格式转换，该步骤已不需要执行\n","\n","\n","---"],"metadata":{"id":"kVD8FDkjYX7S"}},{"cell_type":"code","source":["!mv /content/chinese_sfted/pytorch_model.bin /content/chinese_sfted/adapter_model.bin"],"metadata":{"id":"uwOd7bhZwQAi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["以下拷贝目录和拷贝zip包操作2选1\n","\n","选项1：拷贝结果目录"],"metadata":{"id":"75Xa1dCJvX4m"}},{"cell_type":"code","source":["!cp -fr /content/chinese_sfted/sft_lora_model /content/drive/MyDrive/"],"metadata":{"id":"oGZMnCMEc-aE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["选项2：\n","可以直接打包后再拷贝到google drive,方便之后的下载等操作"],"metadata":{"id":"LK8Z2pKovMTQ"}},{"cell_type":"code","source":["!zip -j -r /content/chinese_sfted.zip /content/chinese_sfted/sft_lora_model"],"metadata":{"id":"h-GVcFpuus7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -fr /content/chinese_sfted.zip /content/drive/MyDrive/"],"metadata":{"id":"kOR3Z0g5-dDe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 关于产出物\n","\n","***deperated：***最新版Chinese-LLaMA-Alpaca已经自动完成了格式转换，该步骤已不需要执行\n","\n","\n","---\n","\n","\n","产出物为Lora权重，目录结构中包含pytorch_model.bin文件，但如果用于合并，实际需要两个额外文件\n","\n","* adapter_model.bin (将pytorch_model.bin重新命名)\n","* adapter_config.json (这个配置文件的主体内容如下，其中参数和我们执行上述精调命令时的参数有对应关系执行可以生成配置文件）\n","\n","如果你调整了参数，请对应调整这里的参数"],"metadata":{"id":"yDXatW0Ao4C5"}},{"cell_type":"code","source":["f = open(\"/content/drive/MyDrive/chinese_sfted/adapter_confjg.json\", \"a\")\n","f.write(\"\"\"{\n","  \"base_model_name_or_path\": \"/content/alpaca-combined\"\n","  \"bias\": \"none\",\n","  \"enable_lora\": null,\n","  \"fan_in_fan_out\": false,\n","  \"inference_mode\": true,\n","  \"init_lora_weights\": true,\n","  \"lora_alpha\": 32,\n","  \"lora_dropout\": 0.05,\n","  \"merge_weights\": false,\n","  \"modules_to_save\": [\n","    \"embed_tokens\",\n","    \"lm_head\"\n","  ],\n","  \"peft_type\": \"LORA\",\n","  \"r\": 8,\n","  \"target_modules\": [\n","    \"q_proj\",\n","    \"v_proj\",\n","    \"k_proj\",\n","    \"o_proj\",\n","    \"gate_proj\",\n","    \"down_proj\",\n","    \"up_proj\"\n","  ],\n","  \"task_type\":\"CAUSAL_LM\"\n","}\"\"\")\n","f.close()"],"metadata":{"id":"laDcRe3V0ObI"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1fKpMX7i6KognPjqmYE6mOqrS3zwE1SEE","timestamp":1685061346926},{"file_id":"1DgBrWVyTaxeRdPkToSAattDHhLGPKl23","timestamp":1684138213170},{"file_id":"1axIgPoThgm-v3rglmRV9QnhVsJKHsHBj","timestamp":1684120777048},{"file_id":"1Eak6azD3MLeb-YsfbP8UZC8wrL1ddIMI","timestamp":1682666840625}],"gpuType":"A100","gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}